 Researchers from MIT, the MIT-IBM Watson AI Lab, and Harvard Medical School have produced a hypothesis that may explain how a transformer could be built using biological elements in the brain . Their hypothesis provides insights that could spark future neuroscience research into how the human brain works .  Transformers operate differently than other neural network models . For self-attention to work, the transformer must keep all the words ready in some form of memory . This didn’t seem biologically possible due to the way neurons communicate . A few years ago scientists studying a machine-learning model (known as a Dense Associated Memory) realized this mechanism could occur in the brain .  The next step for the researchers is to make the leap from theory to practice . “The potential of unleashing the computational power of the other half of our brain is enormous,” says Rutgers University's Konstantinos Michmizos .