{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c11a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\radhi\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7631bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.28.0 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (4.28.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from transformers==4.28.0) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.28.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from requests->transformers==4.28.0) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b311ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup # For web scraping\n",
    "import requests # For sending http requests for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f68546c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# Load summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d072be",
   "metadata": {},
   "source": [
    "## Getting content from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fb6f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PDF link.\n",
    "URL = \"https://news.mit.edu/2023/ai-models-astrocytes-role-brain-0815\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa991b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61203aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of BeautifulSoup by passing in the text.\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "# Get the required text by searching for h1 and p tags.\n",
    "results = soup.find_all(['h1', 'p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4497a906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"tle-search--suggested-results--feedback\"><a class=\"tle-search--suggested-results--feedback--link\" href=\"http://web.mit.edu/feedback\">Suggestions or feedback?</a></p>,\n",
       " <h1><span itemprop=\"name headline\">AI models are powerful, but are they biologically plausible?</span> \n",
       " </h1>,\n",
       " <p>\n",
       "     Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a \n",
       "     <a href=\"http://creativecommons.org/licenses/by-nc-nd/3.0/\" target=\"_blank\">Creative Commons Attribution Non-Commercial No Derivatives license</a>.\n",
       "     You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided \n",
       "     below, credit the images to \"MIT.\" \n",
       "   </p>,\n",
       " <p class=\"news-article--images-gallery--nav--inner\">\n",
       " <button class=\"news-article--images-gallery--nav--button news-article--images-gallery--nav--button--previous\"><svg class=\"arrow--point-west--slider\" data-name=\"Group 2041\" viewbox=\"0 0 16.621 30.183\" width=\"0\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       " <clippath id=\"clip-path\">\n",
       " <path d=\"M0-126.293H16.621V-96.11H0Z\" data-name=\"Path 494\" id=\"Path_494\" transform=\"translate(0 126.293)\"></path>\n",
       " </clippath>\n",
       " </defs>\n",
       " <g clip-path=\"url(#clip-path)\" data-name=\"Group 112\" id=\"Group_112\">\n",
       " <g data-name=\"Group 111\" id=\"Group_111\" transform=\"translate(0 0)\">\n",
       " <path d=\"M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448\" data-name=\"Path 493\" id=\"Path_493\" transform=\"translate(63.146 126.293)\"></path>\n",
       " </g>\n",
       " </g>\n",
       " </svg>\n",
       " <span class=\"visually-hidden\">Previous image</span></button>\n",
       " <button class=\"news-article--images-gallery--nav--button news-article--images-gallery--nav--button--next\"><span class=\"visually-hidden\">Next image</span><svg class=\"arrow--point-east--slider\" viewbox=\"0 0 16.621 30.183\" width=\"0\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       " <clippath id=\"clip-path\">\n",
       " <path d=\"M0-126.293H16.621V-96.11H0Z\" data-name=\"Path 496\" id=\"Path_496\" transform=\"translate(0 126.293)\"></path>\n",
       " </clippath>\n",
       " </defs>\n",
       " <g data-name=\"Group 2042\" id=\"Group_2042\" transform=\"translate(16.621 30.183) rotate(180)\">\n",
       " <g clip-path=\"url(#clip-path)\" data-name=\"Group 115\" id=\"Group_115\">\n",
       " <g data-name=\"Group 114\" id=\"Group_114\" transform=\"translate(0 0)\">\n",
       " <path d=\"M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448\" data-name=\"Path 495\" id=\"Path_495\" transform=\"translate(63.146 126.293)\"></path>\n",
       " </g>\n",
       " </g>\n",
       " </g>\n",
       " </svg></button>\n",
       " </p>,\n",
       " <p>Artificial neural networks, ubiquitous machine-learning models that can be trained to complete many tasks, are so called because their architecture is inspired by the way biological neurons process information in the human brain.</p>,\n",
       " <p>About six years ago, scientists discovered a new type of more powerful neural network model known as a transformer. These models can achieve unprecedented performance, such as by generating text from prompts with near-human-like accuracy. A transformer underlies AI systems such as ChatGPT and Bard, for example. While incredibly effective, transformers are also mysterious: Unlike with other brain-inspired neural network models, it hasn’t been clear how to build them using biological components.</p>,\n",
       " <p>Now, researchers from MIT, the MIT-IBM Watson AI Lab, and Harvard Medical School have produced a hypothesis that may explain how a transformer could be built using biological elements in the brain. They suggest that a biological network composed of neurons and other brain cells called astrocytes could perform the same core computation as a transformer.</p>,\n",
       " <p>Recent research has shown that astrocytes, non-neuronal cells that are abundant in the brain, communicate with neurons and play a role in some physiological processes, like regulating blood flow. But scientists still lack a clear understanding of what these cells do computationally.</p>,\n",
       " <p>With <a href=\"https://doi.org/10.1073/pnas.2219150120\" target=\"_blank\">the new study</a>, published this week in open-access format in the<em> Proceedings of the National Academy of Sciences</em>, the researchers explored the role astrocytes play in the brain from a computational perspective, and crafted a mathematical model that shows how they could be used, along with neurons, to build a biologically plausible transformer.</p>,\n",
       " <p>Their hypothesis provides insights that could spark future neuroscience research into how the human brain works. At the same time, it could help machine-learning researchers explain why transformers are so successful across a diverse set of complex tasks.</p>,\n",
       " <p>“The brain is far superior to even the best artificial neural networks that we have developed, but we don’t really know exactly how the brain works. There is scientific value in thinking about connections between biological hardware and large-scale artificial intelligence networks. This is neuroscience for AI and AI for neuroscience,” says Dmitry Krotov, a research staff member at the MIT-IBM Watson AI Lab and senior author of the research paper.</p>,\n",
       " <p>Joining Krotov on the paper are lead author Leo Kozachkov, a postdoc in the MIT Department of Brain and Cognitive Sciences; and Ksenia V. Kastanenka, an assistant professor of neurobiology at Harvard Medical School and an assistant investigator at the Massachusetts General Research Institute.  </p>,\n",
       " <p><strong>A biological impossibility becomes plausible</strong></p>,\n",
       " <p>Transformers operate differently than other neural network models. For instance, a recurrent neural network trained for natural language processing would compare each word in a sentence to an internal state determined by the previous words. A transformer, on the other hand, compares all the words in the sentence at once to generate a prediction, a process called self-attention.</p>,\n",
       " <p>For self-attention to work, the transformer must keep all the words ready in some form of memory, Krotov explains, but this didn’t seem biologically possible due to the way neurons communicate.</p>,\n",
       " <p>However, a few years ago scientists studying a slightly different type of machine-learning model (known as a Dense Associated Memory) realized that this self-attention mechanism could occur in the brain, but only if there were communication between at least three neurons.</p>,\n",
       " <p>“The number three really popped out to me because it is known in neuroscience that these cells called astrocytes, which are not neurons, form three-way connections with neurons, what are called tripartite synapses,” Kozachkov says.</p>,\n",
       " <p>When two neurons communicate, a presynaptic neuron sends chemicals called neurotransmitters across the synapse that connects it to a postsynaptic neuron. Sometimes, an astrocyte is also connected — it wraps a long, thin tentacle around the synapse, creating a tripartite (three-part) synapse. One astrocyte may form millions of tripartite synapses.</p>,\n",
       " <p>The astrocyte collects some neurotransmitters that flow through the synaptic junction. At some point, the astrocyte can signal back to the neurons. Because astrocytes operate on a much longer time scale than neurons — they create signals by slowly elevating their calcium response and then decreasing it — these cells can hold and integrate information communicated to them from neurons. In this way, astrocytes can form a type of memory buffer, Krotov says.</p>,\n",
       " <p>“If you think about it from that perspective, then astrocytes are extremely natural for precisely the computation we need to perform the attention operation inside transformers,” he adds.</p>,\n",
       " <p><strong>Building a neuron-astrocyte network</strong></p>,\n",
       " <p>With this insight, the researchers formed their hypothesis that astrocytes could play a role in how transformers compute. Then they set out to build a mathematical model of a neuron-astrocyte network that would operate like a transformer.</p>,\n",
       " <p>They took the core mathematics that comprise a transformer and developed simple biophysical models of what astrocytes and neurons do when they communicate in the brain, based on a deep dive into the literature and guidance from neuroscientist collaborators.</p>,\n",
       " <p>Then they combined the models in certain ways until they arrived at an equation of a neuron-astrocyte network that describes a transformer’s self-attention.</p>,\n",
       " <p>“Sometimes, we found that certain things we wanted to be true couldn’t be plausibly implemented. So, we had to think of workarounds. There are some things in the paper that are very careful approximations of the transformer architecture to be able to match it in a biologically plausible way,” Kozachkov says.</p>,\n",
       " <p>Through their analysis, the researchers showed that their biophysical neuron-astrocyte network theoretically matches a transformer. In addition, they conducted numerical simulations by feeding images and paragraphs of text to transformer models and comparing the responses to those of their simulated neuron-astrocyte network. Both responded to the prompts in similar ways, confirming their theoretical model.</p>,\n",
       " <p>“Having remained electrically silent for over a century of brain recordings, astrocytes are one of the most abundant, yet less explored, cells in the brain. The potential of unleashing the computational power of the other half of our brain is enormous,” says Konstantinos Michmizos, associate professor of computer science at Rutgers University, who was not involved with this work. “This study opens up a fascinating iterative loop, from understanding how intelligent behavior may truly emerge in the brain, to translating disruptive hypotheses into new tools that exhibit human-like intelligence.”</p>,\n",
       " <p>The next step for the researchers is to make the leap from theory to practice. They hope to compare the model’s predictions to those that have been observed in biological experiments, and use this knowledge to refine, or possibly disprove, their hypothesis.</p>,\n",
       " <p>In addition, one implication of their study is that astrocytes may be involved in long-term memory, since the network needs to store information to be able act on it in the future. Additional research could investigate this idea further, Krotov says.</p>,\n",
       " <p>“For a lot of reasons, astrocytes are extremely important for cognition and behavior, and they operate in fundamentally different ways from neurons. My biggest hope for this paper is that it catalyzes a bunch of research in computational neuroscience toward glial cells, and in particular, astrocytes,” adds Kozachkov.</p>,\n",
       " <p>This research was supported, in part, by the BrightFocus Foundation and the National Institute of Health.</p>,\n",
       " <p class=\"news-article--archives--nav\">\n",
       " <button class=\"news-article--archives--nav--button news-article--archives--nav--button--previous\"><svg class=\"arrow--point-west--slider\" data-name=\"Group 2041\" viewbox=\"0 0 16.621 30.183\" width=\"0\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       " <clippath id=\"clip-path\">\n",
       " <path d=\"M0-126.293H16.621V-96.11H0Z\" data-name=\"Path 494\" id=\"Path_494\" transform=\"translate(0 126.293)\"></path>\n",
       " </clippath>\n",
       " </defs>\n",
       " <g clip-path=\"url(#clip-path)\" data-name=\"Group 112\" id=\"Group_112\">\n",
       " <g data-name=\"Group 111\" id=\"Group_111\" transform=\"translate(0 0)\">\n",
       " <path d=\"M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448\" data-name=\"Path 493\" id=\"Path_493\" transform=\"translate(63.146 126.293)\"></path>\n",
       " </g>\n",
       " </g>\n",
       " </svg>\n",
       " <span class=\"visually-hidden\">Previous item</span></button>\n",
       " <button class=\"news-article--archives--nav--button news-article--archives--nav--button--next\"><span class=\"visually-hidden\">Next item</span><svg class=\"arrow--point-east--slider\" viewbox=\"0 0 16.621 30.183\" width=\"0\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       " <clippath id=\"clip-path\">\n",
       " <path d=\"M0-126.293H16.621V-96.11H0Z\" data-name=\"Path 496\" id=\"Path_496\" transform=\"translate(0 126.293)\"></path>\n",
       " </clippath>\n",
       " </defs>\n",
       " <g data-name=\"Group 2042\" id=\"Group_2042\" transform=\"translate(16.621 30.183) rotate(180)\">\n",
       " <g clip-path=\"url(#clip-path)\" data-name=\"Group 115\" id=\"Group_115\">\n",
       " <g data-name=\"Group 114\" id=\"Group_114\" transform=\"translate(0 0)\">\n",
       " <path d=\"M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448\" data-name=\"Path 495\" id=\"Path_495\" transform=\"translate(63.146 126.293)\"></path>\n",
       " </g>\n",
       " </g>\n",
       " </g>\n",
       " </svg></button>\n",
       " </p>,\n",
       " <p class=\"news-article--recent-news--full-story visually-hidden\">\n",
       " <a class=\"news-article--recent-news--full-story-link\" href=\"/2023/mit-code-good-club-works-local-nonprofits-0816\">Read full story</a> →\n",
       "             </p>,\n",
       " <p class=\"news-article--recent-news--full-story visually-hidden\">\n",
       " <a class=\"news-article--recent-news--full-story-link\" href=\"/2023/studying-how-children-learn-words-no-meaning-0816\">Read full story</a> →\n",
       "             </p>,\n",
       " <p class=\"news-article--recent-news--full-story visually-hidden\">\n",
       " <a class=\"news-article--recent-news--full-story-link\" href=\"/2023/summer-research-opportunity-can-be-springboard-advanced-studies-0816\">Read full story</a> →\n",
       "             </p>,\n",
       " <p class=\"news-article--recent-news--full-story visually-hidden\">\n",
       " <a class=\"news-article--recent-news--full-story-link\" href=\"/2023/snapshot-cancer-vaccine-development-0815\">Read full story</a> →\n",
       "             </p>,\n",
       " <p class=\"news-article--recent-news--full-story visually-hidden\">\n",
       " <a class=\"news-article--recent-news--full-story-link\" href=\"/2023/simple-superconducting-device-could-dramatically-cut-energy-use-computing-other-important-0815\">Read full story</a> →\n",
       "             </p>,\n",
       " <p class=\"news-article--recent-news--full-story visually-hidden\">\n",
       " <a class=\"news-article--recent-news--full-story-link\" href=\"/2023/3-questions-noah-nathan-and-ariel-white-global-diversity-lab-summer-internship-program-0815\">Read full story</a> →\n",
       "             </p>,\n",
       " <p>This website is managed by the MIT News Office, part of the <a href=\"http://comms.mit.edu\" target=\"_blank\">Institute Office of Communications</a>.</p>,\n",
       " <p><a class=\"mit-name\" href=\"http://web.mit.edu\" target=\"_blank\">Massachusetts Institute of Technology</a><br/>77 Massachusetts Avenue, Cambridge, MA, USA</p>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fff6e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting and concatenating text from the tags into one single block.\n",
    "text = [result.text for result in results]\n",
    "article = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef6e8203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Suggestions or feedback? AI models are powerful, but are they biologically plausible? \\n \\n    Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a \\n    Creative Commons Attribution Non-Commercial No Derivatives license.\\n    You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided \\n    below, credit the images to \"MIT.\" \\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPrevious image\\nNext image\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Artificial neural networks, ubiquitous machine-learning models that can be trained to complete many tasks, are so called because their architecture is inspired by the way biological neurons process information in the human brain. About six years ago, scientists discovered a new type of more powerful neural network model known as a transformer. These models can achieve unprecedented performance, such as by generating text from prompts with near-human-like accuracy. A transformer underlies AI systems such as ChatGPT and Bard, for example. While incredibly effective, transformers are also mysterious: Unlike with other brain-inspired neural network models, it hasn’t been clear how to build them using biological components. Now, researchers from MIT, the MIT-IBM Watson AI Lab, and Harvard Medical School have produced a hypothesis that may explain how a transformer could be built using biological elements in the brain. They suggest that a biological network composed of neurons and other brain cells called astrocytes could perform the same core computation as a transformer. Recent research has shown that astrocytes, non-neuronal cells that are abundant in the brain, communicate with neurons and play a role in some physiological processes, like regulating blood flow. But scientists still lack a clear understanding of what these cells do computationally. With the new study, published this week in open-access format in the Proceedings of the National Academy of Sciences, the researchers explored the role astrocytes play in the brain from a computational perspective, and crafted a mathematical model that shows how they could be used, along with neurons, to build a biologically plausible transformer. Their hypothesis provides insights that could spark future neuroscience research into how the human brain works. At the same time, it could help machine-learning researchers explain why transformers are so successful across a diverse set of complex tasks. “The brain is far superior to even the best artificial neural networks that we have developed, but we don’t really know exactly how the brain works. There is scientific value in thinking about connections between biological hardware and large-scale artificial intelligence networks. This is neuroscience for AI and AI for neuroscience,” says Dmitry Krotov, a research staff member at the MIT-IBM Watson AI Lab and senior author of the research paper. Joining Krotov on the paper are lead author Leo Kozachkov, a postdoc in the MIT Department of Brain and Cognitive Sciences; and Ksenia V. Kastanenka, an assistant professor of neurobiology at Harvard Medical School and an assistant investigator at the Massachusetts General Research Institute. \\xa0 A biological impossibility becomes plausible Transformers operate differently than other neural network models. For instance, a recurrent neural network trained for natural language processing would compare each word in a sentence to an internal state determined by the previous words. A transformer, on the other hand, compares all the words in the sentence at once to generate a prediction, a process called self-attention. For self-attention to work, the transformer must keep all the words ready in some form of memory, Krotov explains, but this didn’t seem biologically possible due to the way neurons communicate. However, a few years ago scientists studying a slightly different type of machine-learning model (known as a Dense Associated Memory) realized that this self-attention mechanism could occur in the brain, but only if there were communication between at least three neurons. “The number three really popped out to me because it is known in neuroscience that these cells called astrocytes, which are not neurons, form three-way connections with neurons, what are called tripartite synapses,” Kozachkov says. When two neurons communicate, a presynaptic neuron sends chemicals called neurotransmitters across the synapse that connects it to a postsynaptic neuron. Sometimes, an astrocyte is also connected — it wraps a long, thin tentacle around the synapse, creating a tripartite (three-part) synapse. One astrocyte may form millions of tripartite synapses. The astrocyte collects some neurotransmitters that flow through the synaptic junction. At some point, the astrocyte can signal back to the neurons. Because astrocytes operate on a much longer time scale than neurons — they create signals by slowly elevating their calcium response and then decreasing it — these cells can hold and integrate information communicated to them from neurons. In this way, astrocytes can form a type of memory buffer, Krotov says. “If you think about it from that perspective, then astrocytes are extremely natural for precisely the computation we need to perform the attention operation inside transformers,” he adds. Building a neuron-astrocyte network With this insight, the researchers formed their hypothesis that astrocytes could play a role in how transformers compute. Then they set out to build a mathematical model of a neuron-astrocyte network that would operate like a transformer. They took the core mathematics that comprise a transformer and developed simple biophysical models of what astrocytes and neurons do when they communicate in the brain, based on a deep dive into the literature and guidance from neuroscientist collaborators. Then they combined the models in certain ways until they arrived at an equation of a neuron-astrocyte network that describes a transformer’s self-attention. “Sometimes, we found that certain things we wanted to be true couldn’t be plausibly implemented. So, we had to think of workarounds. There are some things in the paper that are very careful approximations of the transformer architecture to be able to match it in a biologically plausible way,” Kozachkov says. Through their analysis, the researchers showed that their biophysical neuron-astrocyte network theoretically matches a transformer. In addition, they conducted numerical simulations by feeding images and paragraphs of text to transformer models and comparing the responses to those of their simulated neuron-astrocyte network. Both responded to the prompts in similar ways, confirming their theoretical model. “Having remained electrically silent for over a century of brain recordings, astrocytes are one of the most abundant, yet less explored, cells in the brain. The potential of unleashing the computational power of the other half of our brain is enormous,” says Konstantinos Michmizos, associate professor of computer science at Rutgers University, who was not involved with this work. “This study opens up a fascinating iterative loop, from understanding how intelligent behavior may truly emerge in the brain, to translating disruptive hypotheses into new tools that exhibit human-like intelligence.” The next step for the researchers is to make the leap from theory to practice. They hope to compare the model’s predictions to those that have been observed in biological experiments, and use this knowledge to refine, or possibly disprove, their hypothesis. In addition, one implication of their study is that astrocytes may be involved in long-term memory, since the network needs to store information to be able act on it in the future. Additional research could investigate this idea further, Krotov says. “For a lot of reasons, astrocytes are extremely important for cognition and behavior, and they operate in fundamentally different ways from neurons. My biggest hope for this paper is that it catalyzes a bunch of research in computational neuroscience toward glial cells, and in particular, astrocytes,” adds Kozachkov. This research was supported, in part, by the BrightFocus Foundation and the National Institute of Health. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPrevious item\\nNext item\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nRead full story →\\n             \\nRead full story →\\n             \\nRead full story →\\n             \\nRead full story →\\n             \\nRead full story →\\n             \\nRead full story →\\n             This website is managed by the MIT News Office, part of the Institute Office of Communications. Massachusetts Institute of Technology77 Massachusetts Avenue, Cambridge, MA, USA'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89264090",
   "metadata": {},
   "source": [
    "## Chunking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d6cdd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking the text in order to be passed to basic transformed model. <eos> tag added to split using the tag to build array.\n",
    "article = article.replace('.', '.<eos>')\n",
    "article = article.replace('?', '?<eos>')\n",
    "article = article.replace('!', '!<eos>')\n",
    "sentences = article.split('<eos>') # Creating an array of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4173e91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Suggestions or feedback?',\n",
       " ' AI models are powerful, but are they biologically plausible?',\n",
       " ' \\n \\n    Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a \\n    Creative Commons Attribution Non-Commercial No Derivatives license.',\n",
       " '\\n    You may not alter the images provided, other than to crop them to size.',\n",
       " ' A credit line must be used when reproducing images; if one is not provided \\n    below, credit the images to \"MIT.',\n",
       " '\" \\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPrevious image\\nNext image\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Artificial neural networks, ubiquitous machine-learning models that can be trained to complete many tasks, are so called because their architecture is inspired by the way biological neurons process information in the human brain.',\n",
       " ' About six years ago, scientists discovered a new type of more powerful neural network model known as a transformer.',\n",
       " ' These models can achieve unprecedented performance, such as by generating text from prompts with near-human-like accuracy.',\n",
       " ' A transformer underlies AI systems such as ChatGPT and Bard, for example.',\n",
       " ' While incredibly effective, transformers are also mysterious: Unlike with other brain-inspired neural network models, it hasn’t been clear how to build them using biological components.',\n",
       " ' Now, researchers from MIT, the MIT-IBM Watson AI Lab, and Harvard Medical School have produced a hypothesis that may explain how a transformer could be built using biological elements in the brain.',\n",
       " ' They suggest that a biological network composed of neurons and other brain cells called astrocytes could perform the same core computation as a transformer.',\n",
       " ' Recent research has shown that astrocytes, non-neuronal cells that are abundant in the brain, communicate with neurons and play a role in some physiological processes, like regulating blood flow.',\n",
       " ' But scientists still lack a clear understanding of what these cells do computationally.',\n",
       " ' With the new study, published this week in open-access format in the Proceedings of the National Academy of Sciences, the researchers explored the role astrocytes play in the brain from a computational perspective, and crafted a mathematical model that shows how they could be used, along with neurons, to build a biologically plausible transformer.',\n",
       " ' Their hypothesis provides insights that could spark future neuroscience research into how the human brain works.',\n",
       " ' At the same time, it could help machine-learning researchers explain why transformers are so successful across a diverse set of complex tasks.',\n",
       " ' “The brain is far superior to even the best artificial neural networks that we have developed, but we don’t really know exactly how the brain works.',\n",
       " ' There is scientific value in thinking about connections between biological hardware and large-scale artificial intelligence networks.',\n",
       " ' This is neuroscience for AI and AI for neuroscience,” says Dmitry Krotov, a research staff member at the MIT-IBM Watson AI Lab and senior author of the research paper.',\n",
       " ' Joining Krotov on the paper are lead author Leo Kozachkov, a postdoc in the MIT Department of Brain and Cognitive Sciences; and Ksenia V.',\n",
       " ' Kastanenka, an assistant professor of neurobiology at Harvard Medical School and an assistant investigator at the Massachusetts General Research Institute.',\n",
       " ' \\xa0 A biological impossibility becomes plausible Transformers operate differently than other neural network models.',\n",
       " ' For instance, a recurrent neural network trained for natural language processing would compare each word in a sentence to an internal state determined by the previous words.',\n",
       " ' A transformer, on the other hand, compares all the words in the sentence at once to generate a prediction, a process called self-attention.',\n",
       " ' For self-attention to work, the transformer must keep all the words ready in some form of memory, Krotov explains, but this didn’t seem biologically possible due to the way neurons communicate.',\n",
       " ' However, a few years ago scientists studying a slightly different type of machine-learning model (known as a Dense Associated Memory) realized that this self-attention mechanism could occur in the brain, but only if there were communication between at least three neurons.',\n",
       " ' “The number three really popped out to me because it is known in neuroscience that these cells called astrocytes, which are not neurons, form three-way connections with neurons, what are called tripartite synapses,” Kozachkov says.',\n",
       " ' When two neurons communicate, a presynaptic neuron sends chemicals called neurotransmitters across the synapse that connects it to a postsynaptic neuron.',\n",
       " ' Sometimes, an astrocyte is also connected — it wraps a long, thin tentacle around the synapse, creating a tripartite (three-part) synapse.',\n",
       " ' One astrocyte may form millions of tripartite synapses.',\n",
       " ' The astrocyte collects some neurotransmitters that flow through the synaptic junction.',\n",
       " ' At some point, the astrocyte can signal back to the neurons.',\n",
       " ' Because astrocytes operate on a much longer time scale than neurons — they create signals by slowly elevating their calcium response and then decreasing it — these cells can hold and integrate information communicated to them from neurons.',\n",
       " ' In this way, astrocytes can form a type of memory buffer, Krotov says.',\n",
       " ' “If you think about it from that perspective, then astrocytes are extremely natural for precisely the computation we need to perform the attention operation inside transformers,” he adds.',\n",
       " ' Building a neuron-astrocyte network With this insight, the researchers formed their hypothesis that astrocytes could play a role in how transformers compute.',\n",
       " ' Then they set out to build a mathematical model of a neuron-astrocyte network that would operate like a transformer.',\n",
       " ' They took the core mathematics that comprise a transformer and developed simple biophysical models of what astrocytes and neurons do when they communicate in the brain, based on a deep dive into the literature and guidance from neuroscientist collaborators.',\n",
       " ' Then they combined the models in certain ways until they arrived at an equation of a neuron-astrocyte network that describes a transformer’s self-attention.',\n",
       " ' “Sometimes, we found that certain things we wanted to be true couldn’t be plausibly implemented.',\n",
       " ' So, we had to think of workarounds.',\n",
       " ' There are some things in the paper that are very careful approximations of the transformer architecture to be able to match it in a biologically plausible way,” Kozachkov says.',\n",
       " ' Through their analysis, the researchers showed that their biophysical neuron-astrocyte network theoretically matches a transformer.',\n",
       " ' In addition, they conducted numerical simulations by feeding images and paragraphs of text to transformer models and comparing the responses to those of their simulated neuron-astrocyte network.',\n",
       " ' Both responded to the prompts in similar ways, confirming their theoretical model.',\n",
       " ' “Having remained electrically silent for over a century of brain recordings, astrocytes are one of the most abundant, yet less explored, cells in the brain.',\n",
       " ' The potential of unleashing the computational power of the other half of our brain is enormous,” says Konstantinos Michmizos, associate professor of computer science at Rutgers University, who was not involved with this work.',\n",
       " ' “This study opens up a fascinating iterative loop, from understanding how intelligent behavior may truly emerge in the brain, to translating disruptive hypotheses into new tools that exhibit human-like intelligence.',\n",
       " '” The next step for the researchers is to make the leap from theory to practice.',\n",
       " ' They hope to compare the model’s predictions to those that have been observed in biological experiments, and use this knowledge to refine, or possibly disprove, their hypothesis.',\n",
       " ' In addition, one implication of their study is that astrocytes may be involved in long-term memory, since the network needs to store information to be able act on it in the future.',\n",
       " ' Additional research could investigate this idea further, Krotov says.',\n",
       " ' “For a lot of reasons, astrocytes are extremely important for cognition and behavior, and they operate in fundamentally different ways from neurons.',\n",
       " ' My biggest hope for this paper is that it catalyzes a bunch of research in computational neuroscience toward glial cells, and in particular, astrocytes,” adds Kozachkov.',\n",
       " ' This research was supported, in part, by the BrightFocus Foundation and the National Institute of Health.',\n",
       " ' \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPrevious item\\nNext item\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\nRead full story →\\n             \\nRead full story →\\n             \\nRead full story →\\n             \\nRead full story →\\n             \\nRead full story →\\n             \\nRead full story →\\n             This website is managed by the MIT News Office, part of the Institute Office of Communications.',\n",
       " ' Massachusetts Institute of Technology77 Massachusetts Avenue, Cambridge, MA, USA']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "971cf1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Limit max word count of a chunk to 500\n",
    "max_chunk = 500\n",
    "\n",
    "current_chunk = 0 \n",
    "chunks = []\n",
    "for sentence in sentences:\n",
    "    if len(chunks) == current_chunk + 1: ## Check if a chunk was already started.\n",
    "        if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk: ## Check if current chunk size + new sentence < max chunk size\n",
    "            chunks[current_chunk].extend(sentence.split(' '))\n",
    "        else:  ## If chunk size exceeds max limit, start new chunk\n",
    "            current_chunk += 1\n",
    "            chunks.append(sentence.split(' '))\n",
    "    else:    ## If new chunk being created, add word by splitting each sentence.\n",
    "        print(current_chunk)\n",
    "        chunks.append(sentence.split(' '))\n",
    "\n",
    "for chunk_id in range(len(chunks)):\n",
    "    chunks[chunk_id] = ' '.join(chunks[chunk_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7015decc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Suggestions or feedback?  AI models are powerful, but are they biologically plausible?  \\n \\n    Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a \\n    Creative Commons Attribution Non-Commercial No Derivatives license. \\n    You may not alter the images provided, other than to crop them to size.  A credit line must be used when reproducing images; if one is not provided \\n    below, credit the images to \"MIT. \" \\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPrevious image\\nNext image\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Artificial neural networks, ubiquitous machine-learning models that can be trained to complete many tasks, are so called because their architecture is inspired by the way biological neurons process information in the human brain.  About six years ago, scientists discovered a new type of more powerful neural network model known as a transformer.  These models can achieve unprecedented performance, such as by generating text from prompts with near-human-like accuracy.  A transformer underlies AI systems such as ChatGPT and Bard, for example.  While incredibly effective, transformers are also mysterious: Unlike with other brain-inspired neural network models, it hasn’t been clear how to build them using biological components.  Now, researchers from MIT, the MIT-IBM Watson AI Lab, and Harvard Medical School have produced a hypothesis that may explain how a transformer could be built using biological elements in the brain.  They suggest that a biological network composed of neurons and other brain cells called astrocytes could perform the same core computation as a transformer.  Recent research has shown that astrocytes, non-neuronal cells that are abundant in the brain, communicate with neurons and play a role in some physiological processes, like regulating blood flow.  But scientists still lack a clear understanding of what these cells do computationally.  With the new study, published this week in open-access format in the Proceedings of the National Academy of Sciences, the researchers explored the role astrocytes play in the brain from a computational perspective, and crafted a mathematical model that shows how they could be used, along with neurons, to build a biologically plausible transformer.  Their hypothesis provides insights that could spark future neuroscience research into how the human brain works.  At the same time, it could help machine-learning researchers explain why transformers are so successful across a diverse set of complex tasks.  “The brain is far superior to even the best artificial neural networks that we have developed, but we don’t really know exactly how the brain works.  There is scientific value in thinking about connections between biological hardware and large-scale artificial intelligence networks.  This is neuroscience for AI and AI for neuroscience,” says Dmitry Krotov, a research staff member at the MIT-IBM Watson AI Lab and senior author of the research paper.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks) # Returns 3\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc02c2",
   "metadata": {},
   "source": [
    "## Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6e7ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing text\n",
    "summary = summarizer(chunks, max_length=150, min_length=30, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79c51cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Researchers from MIT, the MIT-IBM Watson AI Lab, and Harvard Medical School have produced a hypothesis that may explain how a transformer could be built using biological elements in the brain . Their hypothesis provides insights that could spark future neuroscience research into how the human brain works .'},\n",
       " {'summary_text': ' Transformers operate differently than other neural network models . For self-attention to work, the transformer must keep all the words ready in some form of memory . This didn’t seem biologically possible due to the way neurons communicate . A few years ago scientists studying a machine-learning model (known as a Dense Associated Memory) realized this mechanism could occur in the brain .'},\n",
       " {'summary_text': \" The next step for the researchers is to make the leap from theory to practice . “The potential of unleashing the computational power of the other half of our brain is enormous,” says Rutgers University's Konstantinos Michmizos .\"}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary  ## Shows 3 different summary for each of the 3 chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c86af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the summary texts into one single text\n",
    "text = ' '.join([summ['summary_text'] for summ in summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3afe9109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Researchers from MIT, the MIT-IBM Watson AI Lab, and Harvard Medical School have produced a hypothesis that may explain how a transformer could be built using biological elements in the brain . Their hypothesis provides insights that could spark future neuroscience research into how the human brain works .  Transformers operate differently than other neural network models . For self-attention to work, the transformer must keep all the words ready in some form of memory . This didn’t seem biologically possible due to the way neurons communicate . A few years ago scientists studying a machine-learning model (known as a Dense Associated Memory) realized this mechanism could occur in the brain .  The next step for the researchers is to make the leap from theory to practice . “The potential of unleashing the computational power of the other half of our brain is enormous,” says Rutgers University's Konstantinos Michmizos .\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c915c776",
   "metadata": {},
   "source": [
    "## Writing summary to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d70a3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output summary to a text file\n",
    "with open('blogsummary.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
